{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de python_ir_nltk_gensim.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyqe_EeQ7aSO"
      },
      "source": [
        "#!/usr/bin/python"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-V1ZFZO8NFu",
        "outputId": "42592114-1399-416c-93f6-feba561208fb"
      },
      "source": [
        "# Importamos las librerias necesarias desde bash para el desarrollo de este codigo\r\n",
        "\r\n",
        "!git clone https://github.com/st1800eafit/st1800_20211.git\r\n",
        "!pip install nltk\r\n",
        "!pip install stop-words\r\n",
        "!pip install pandas\r\n",
        "!pip install gensim"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'st1800_20211' already exists and is not an empty directory.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: stop-words in /usr/local/lib/python3.7/dist-packages (2018.7.23)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo5yMUId8Pfn"
      },
      "source": [
        "# Importamos las librerias necesarias desde Python para el desarrollo de este codigo\r\n",
        "\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import re \r\n",
        "import nltk \r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8mCwIhCDkGJ",
        "outputId": "bf0a837b-fadc-48a9-ce25-a5be6f8e37e8"
      },
      "source": [
        "# Dede la libreria de NLTK necesitaremos estos parametros para realizar la limpieza o preparacion de los documentos\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvxmbYns8TK-"
      },
      "source": [
        "# Abrimos los documentos .txt que en este caso utilizaremos y adicionalmente, abrimos el documento de xml. A este ultimo se le realizara una limpieza antes de tokenizar. Para asi remover las etiquetas\r\n",
        "\r\n",
        "files_location = os.path.join(\"/\",\"content\",\"st1800_20211\",\"datasets\", \"papers_sample_pdf/\")\r\n",
        "output_file_location = os.path.join(\"/\",\"content/\")\r\n",
        "output_file = \"scikit_model_vectorized.sav\"\r\n",
        "\r\n",
        "filenames = glob.glob(files_location+\"*.txt\")\r\n",
        "corpus_per_document = []\r\n",
        "\r\n",
        "for file in filenames:\r\n",
        "  doc_corpus = open(file, \"r\").read()\r\n",
        "  corpus_per_document.append(doc_corpus)\r\n",
        "\r\n",
        "# Adicionamos el XML sin tags\r\n",
        "filename_xml = glob.glob(files_location+\"*.xml\")\r\n",
        "xml_file = open (filename_xml[0], \"r\").read()\r\n",
        "xml_file = re.sub('<[^>]*>', \"\", xml_file)\r\n",
        "corpus_per_document.append(xml_file)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjQrH4fU8WS4",
        "outputId": "c3abdab4-c299-42ea-f708-317f489c4b56"
      },
      "source": [
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from gensim import corpora, models\r\n",
        "\r\n",
        "# Aplicaremos la tecnica de TF-IDF pero en Gensim. Primero tokenizaremos los documentos con NLTK\r\n",
        "\r\n",
        "Porter_Stemmer = PorterStemmer()\r\n",
        "\r\n",
        "cleaned_corpus_per_document = []\r\n",
        "stop_words_nltk = set(stopwords.words('english'))\r\n",
        "\r\n",
        "#Primero preparamos cada elemento del corpus\r\n",
        "for document in corpus_per_document:\r\n",
        "  clean_document = nltk.word_tokenize(document)\r\n",
        "  clean_document = [re.sub(r'[^A-Za-z0-9]+','',token) for token in clean_document]\r\n",
        "  clean_document = [token.lower() for token in clean_document if len(token)>1]\r\n",
        "  clean_document = [token for token in clean_document if token not in stop_words_nltk]\r\n",
        "  clean_document = [Porter_Stemmer.stem(token) for token in clean_document]\r\n",
        "  cleaned_corpus_per_document.append(clean_document)\r\n",
        "\r\n",
        "#Luego aplicamos Gensim creando el bag of words\r\n",
        "bow = corpora.Dictionary(cleaned_corpus_per_document)\r\n",
        "#print(bow.token2id)\r\n",
        "corpus = [bow.doc2bow(text) for text in cleaned_corpus_per_document]\r\n",
        "\r\n",
        "#Con este mecanismo podremos guardar el corpus procesado\r\n",
        "corpora.MmCorpus.serialize('/content/docs_corpus.mm', corpus)\r\n",
        "\r\n",
        "#Ahora aplicaremos TF-IDF. Podemros obesrvar que el resultado seria una pareja, con el numero de ocurrencias y el valor del IDF\r\n",
        "tfidf = models.TfidfModel(corpus)\r\n",
        "\r\n",
        "#for document in tfidf[corpus]:\r\n",
        "#  print(document)\r\n",
        "\r\n",
        "for i in range (6):\r\n",
        "  print(tfidf[corpus][i][0:5])\r\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.010450743622335182), (1, 0.0019285292620283823), (2, 0.00336129584950534), (3, 0.005225371811167591), (4, 0.0071720187885617225)]\n",
            "[(6, 0.00684812475308699), (48, 0.006262237830021386), (49, 0.0009783035361552843), (59, 0.002087412610007129), (84, 0.0009783035361552843)]\n",
            "[(1, 0.0022906146563204596), (2, 0.009648266556126443), (3, 0.0031032231377177088), (4, 0.0369138479059912), (5, 0.015516115688588544)]\n",
            "[(1, 0.0008246360431166116), (2, 0.0019163796741599247), (4, 0.002044495879634551), (6, 0.003593211889049859), (11, 0.002044495879634551)]\n",
            "[(2, 0.0007132080749496942), (20, 0.0015217767024125251), (46, 0.0015217767024125251), (47, 0.00736560203941613), (54, 0.0021396242248490826)]\n",
            "[(1, 0.0049914801450228405), (2, 0.0018124607769656727), (4, 0.002320355599970522), (6, 0.0025374450877519417), (11, 0.002320355599970522)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXTVnMLHF8_O"
      },
      "source": [
        "# Primeros ubicamos nuestro corpus en un dataframe para nuestra siguiente implementacion\r\n",
        "\r\n",
        "df = pd.DataFrame(cleaned_corpus_per_document)\r\n",
        "df_clean = df.dropna().drop_duplicates()\r\n",
        "final_bow = df_clean.values.tolist()\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X-I3rUPGXMb",
        "outputId": "59b9d739-b383-4820-c0dc-4ec17d0d19dd"
      },
      "source": [
        "# Utilizaremos la libreria de word2Vec de Gensim y adicionalmente, evidenciaremos cuantos nucleos tenemos disponibles en nuestro procesamiento\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "cores = multiprocessing.cpu_count()\r\n",
        "\r\n",
        "cores"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiPO2o32Hvl0"
      },
      "source": [
        "# Creamos el modelo con los siguientes parametros, entre ellos que solo considere las palabras con minimmo 5 ocurrencias, un ritmo de aprendizaje de 0.03, entre otrods.\r\n",
        "w2v_model = Word2Vec(min_count=5,\r\n",
        "                     window=2,\r\n",
        "                     size=300,\r\n",
        "                     sample=6e-5, \r\n",
        "                     alpha=0.03, \r\n",
        "                     min_alpha=0.0007, \r\n",
        "                     negative=20,\r\n",
        "                     workers=cores-1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI-NeeNtLUsC"
      },
      "source": [
        "# Permitimos que el modelo adquiera el vocabulario ingresado\r\n",
        "w2v_model.build_vocab(final_bow, progress_per=10000)\r\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93nS3--kLdOO",
        "outputId": "cf4e63f6-5933-413b-9e67-e9fe840b2324"
      },
      "source": [
        "# Entrenamos el modelo y evidenciamos el tiempo que se demoro en entrenarse.\r\n",
        "w2v_model.train(final_bow, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 3846450)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHWux6AOLkHH"
      },
      "source": [
        "# Finalmente podremos guardar los parametros del modelo\r\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzTj3sdjL7Zq",
        "outputId": "131ddb7e-eed5-470b-f311-c4c409d8a31e"
      },
      "source": [
        "# Probamos el modelo. Aqui podremos observar que dada la plabra comput, la palabra que mas probabilidad tiene para que le siga es science, lo cual tiene sentido.\r\n",
        "w2v_model.wv.most_similar(positive=[\"comput\"])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('scienc', 0.9999433159828186),\n",
              " ('mathemat', 0.9999307990074158),\n",
              " ('combinator', 0.9999194741249084),\n",
              " ('discret', 0.9999142289161682),\n",
              " ('theori', 0.9999040961265564),\n",
              " ('geometri', 0.9999023079872131),\n",
              " ('structur', 0.9998992681503296),\n",
              " ('parallel', 0.999896228313446),\n",
              " ('michael', 0.999894380569458),\n",
              " ('68q17', 0.9998940825462341)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl3-JdLOMB9E"
      },
      "source": [
        "# Asi podremos guardar el modelo para no tener que realizarlo nuevamente\r\n",
        "w2v_model.save(\"/content/gensim_model.model\")\r\n",
        "\r\n",
        "# Adicionalmente de esta manera podremos cargarlo para nuestro analisis\r\n",
        "loaded_w2d_model = Word2Vec.load(\"/content/gensim_model.model\")"
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}
